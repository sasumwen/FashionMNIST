{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNH0rTRvaM2qYuUkWGcs0xM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sasumwen/FashionMNIST/blob/main/fashion_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mroR8dztW9cP"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sklearn\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels)= fashion_mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usN62lHMXRUW",
        "outputId": "69b1bb0c-a1cc-443d-d62a-48507d3059e9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0"
      ],
      "metadata": {
        "id": "Jn_ytEzTbVC9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select index\n",
        "index= 84\n",
        "\n",
        "# Set Number of characters per row when printing\n",
        "np.set_printoptions(linewidth=320)\n",
        "\n",
        "# print the label and image\n",
        "print(f'Label: {train_labels[index]}')\n",
        "print(f'\\n Image Pixel Array:\\n {train_images[index]}')\n",
        "\n",
        "# visualize the image\n",
        "plt.imshow(train_images[index], cmap=\"Greys\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 866
        },
        "id": "YA8QtulDZ-WM",
        "outputId": "2a3a33d9-8a55-48d4-d8c6-9b419ac62ef6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: 9\n",
            "\n",
            " Image Pixel Array:\n",
            " [[0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.01176471 0.01176471 0.00784314 0.00392157 0.         0.         0.         0.         0.         0.0627451  0.43137255 0.59215686 0.75686275 0.32156863 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.00392157 0.         0.         0.         0.         0.         0.         0.01960784 0.         0.         0.19215686 0.78039216 1.         1.         0.81568627 0.85098039 0.61176471 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.22745098 0.5372549  0.31764706 0.         0.         0.         0.61568627 0.94901961 0.90196078 0.86666667 0.85882353 0.83921569 0.89411765 0.5372549  0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.01960784 0.         0.16862745 0.75686275 0.81176471 0.87058824 0.64705882 0.         0.78431373 0.95294118 0.86666667 0.89019608 0.9254902  0.9254902  0.88235294 0.92156863 0.37647059 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.02745098 0.         0.17647059 0.90196078 0.85882353 0.81176471 0.89803922 0.83137255 0.92156863 0.86666667 0.96862745 0.9372549  0.92156863 0.89803922 0.8745098  0.92156863 0.41176471 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.00392157 0.00784314 0.         0.0745098  0.8745098  0.91764706 0.91764706 0.90980392 0.83137255 0.77254902 0.84705882 0.93333333 0.89803922 0.91372549 0.90196078 0.8745098  0.91764706 0.45098039 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.01176471 0.         0.08627451 0.87058824 0.89019608 0.91764706 0.94509804 0.78039216 0.83921569 0.85490196 0.93333333 0.9372549  0.90588235 0.91372549 0.86666667 0.91764706 0.50588235 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.00784314 0.         0.0627451  1.         0.8745098  0.90980392 0.94509804 0.82352941 0.84705882 0.8745098  0.91764706 0.9254902  0.90196078 0.91372549 0.87058824 0.91764706 0.51764706 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.00784314 0.         0.04705882 0.99607843 0.85882353 0.8745098  0.91764706 0.78823529 0.78823529 0.84705882 0.89411765 0.9372549  0.90196078 0.91372549 0.85098039 0.91764706 0.5372549  0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.01568627 0.         0.03529412 1.         0.8745098  0.8745098  0.90588235 0.81176471 0.82352941 0.8745098  0.89019608 0.93333333 0.89803922 0.91372549 0.85490196 0.90980392 0.61960784 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.01568627 0.         0.         1.         0.90588235 0.92156863 0.94901961 0.79607843 0.83921569 0.90196078 0.90588235 0.92941176 0.89803922 0.90980392 0.85490196 0.90980392 0.56078431 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.84313725 0.92156863 0.84313725 0.89803922 0.79215686 0.87058824 0.85882353 0.84705882 0.93333333 0.90980392 0.89803922 0.84705882 0.91764706 0.49411765 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.00392157 0.         0.24313725 1.         0.83137255 0.87843137 0.77254902 0.84313725 0.84313725 0.85490196 0.94509804 0.90980392 0.89411765 0.85882353 0.91372549 0.71372549 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.00392157 0.         0.         0.         0.76078431 0.97254902 0.88627451 0.89019608 0.86666667 0.84705882 0.83137255 0.8745098  0.9372549  0.89019608 0.89411765 0.88627451 0.89803922 0.90588235 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.01568627 0.01176471 0.         0.         0.         0.         0.04313725 0.44705882 0.94901961 0.93333333 0.98039216 0.91764706 0.89803922 0.83921569 0.85882353 0.89411765 0.90588235 0.89803922 0.89803922 0.89411765 0.89019608 1.         0.02745098]\n",
            " [0.         0.         0.01176471 0.01176471 0.         0.         0.         0.         0.         0.25882353 0.6745098  0.99607843 1.         0.83921569 0.78039216 0.91764706 0.95294118 0.83529412 0.86666667 0.87058824 0.89019608 0.91764706 0.91372549 0.90196078 0.89803922 0.90980392 1.         0.04705882]\n",
            " [0.         0.         0.         0.         0.         0.06666667 0.25098039 0.49803922 0.80392157 1.         0.86666667 0.85098039 0.83137255 0.94901961 0.99215686 0.94901961 0.85882353 0.77254902 0.85882353 0.89803922 0.92156863 0.91764706 0.91372549 0.90980392 0.90980392 0.94117647 0.90980392 0.        ]\n",
            " [0.         0.         0.         0.25098039 0.74509804 0.94509804 1.         0.94117647 0.87058824 0.75686275 0.74509804 0.79215686 0.83529412 0.84705882 0.90196078 0.90980392 0.82745098 0.8        0.87058824 0.89803922 0.92941176 0.94117647 0.9254902  0.92156863 0.90980392 0.95294118 0.79607843 0.        ]\n",
            " [0.         0.31764706 0.83529412 0.70980392 0.75686275 0.71764706 0.6        0.65098039 0.71372549 0.71372549 0.67843137 0.70588235 0.72941176 0.77647059 0.70980392 0.75294118 0.8627451  0.89019608 0.82745098 0.9372549  0.95686275 0.9254902  0.94509804 0.93333333 0.9254902  0.92941176 1.         0.05098039]\n",
            " [0.         0.62745098 0.81568627 0.76862745 0.85882353 0.83529412 0.76470588 0.88627451 0.88235294 0.91372549 0.95686275 0.8745098  0.88627451 0.95294118 0.87843137 0.91764706 0.93333333 0.84705882 0.89803922 0.95686275 0.9254902  0.92941176 0.9254902  0.9254902  0.92941176 0.91764706 1.         0.16862745]\n",
            " [0.         0.79215686 0.80392157 0.81176471 0.81176471 0.8        0.75686275 0.84313725 0.83921569 0.83921569 0.85490196 0.89411765 0.89803922 0.8745098  0.89803922 0.91372549 0.93333333 0.97254902 0.96862745 0.94509804 0.95294118 0.95686275 0.94901961 0.94117647 0.94901961 0.92941176 1.         0.0745098 ]\n",
            " [0.         0.82352941 0.88235294 0.8627451  0.92941176 0.92941176 0.92941176 0.94901961 0.94509804 0.94901961 0.94901961 0.9254902  0.93333333 0.94117647 0.92156863 0.9372549  0.94901961 0.9254902  0.89411765 0.90196078 0.92941176 0.90980392 0.91372549 0.90980392 0.90196078 0.88235294 1.         0.28235294]\n",
            " [0.42352941 0.84705882 0.77254902 0.79607843 0.76078431 0.77647059 0.79607843 0.81176471 0.83529412 0.82352941 0.85098039 0.84705882 0.83921569 0.83529412 0.8627451  0.83137255 0.82352941 0.85490196 0.84705882 0.83921569 0.84705882 0.83921569 0.83921569 0.83137255 0.83921569 0.84705882 0.99607843 0.1372549 ]\n",
            " [0.35686275 1.         0.8745098  1.         1.         1.         1.         0.96470588 0.94509804 0.96078431 0.96862745 0.98823529 1.         0.99607843 1.         1.         1.         1.         1.         1.         1.         1.         1.         0.99607843 0.98039216 0.91764706 1.         0.01568627]\n",
            " [0.         0.05098039 0.22352941 0.38431373 0.4        0.40784314 0.54117647 0.64705882 0.67843137 0.65098039 0.6627451  0.60392157 0.54901961 0.5254902  0.53333333 0.5254902  0.43529412 0.42745098 0.41960784 0.44313725 0.45098039 0.49411765 0.52941176 0.67843137 0.60784314 0.58431373 0.60784314 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f4f9c0b2460>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(84)\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape = (28,28)),\n",
        "    tf.keras.layers.Dense(128, activation= \"relu\"),\n",
        "    tf.keras.layers.Dense(10, activation = \"softmax\")\n",
        "])"
      ],
      "metadata": {
        "id": "lNWMsxB1X5j_"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(84)\n",
        "cb= tf.keras.callbacks.EarlyStopping(monitor = \"accuracy\", patience = 3)"
      ],
      "metadata": {
        "id": "n4U3b62neFvy"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(84)\n",
        "model.compile(loss = tf.keras.losses.sparse_categorical_crossentropy,\n",
        "              optimizer = tf.keras.optimizers.Adam(),\n",
        "              metrics =[\"accuracy\"])"
      ],
      "metadata": {
        "id": "k9r-0aS_YJ4y"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(tf.expand_dims(train_images, -1), train_labels, epochs = 500, callbacks = [cb])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zoBrQJscalb",
        "outputId": "ddae4462-07ee-433d-ef7e-86afb28c8868"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4986 - accuracy: 0.8235\n",
            "Epoch 2/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3747 - accuracy: 0.8654\n",
            "Epoch 3/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3376 - accuracy: 0.8763\n",
            "Epoch 4/500\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.3146 - accuracy: 0.8846\n",
            "Epoch 5/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2971 - accuracy: 0.8913\n",
            "Epoch 6/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2831 - accuracy: 0.8955\n",
            "Epoch 7/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2680 - accuracy: 0.9000\n",
            "Epoch 8/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2585 - accuracy: 0.9023\n",
            "Epoch 9/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2481 - accuracy: 0.9068\n",
            "Epoch 10/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2402 - accuracy: 0.9099\n",
            "Epoch 11/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2310 - accuracy: 0.9126\n",
            "Epoch 12/500\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2231 - accuracy: 0.9169\n",
            "Epoch 13/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2186 - accuracy: 0.9186\n",
            "Epoch 14/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2092 - accuracy: 0.9218\n",
            "Epoch 15/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2040 - accuracy: 0.9244\n",
            "Epoch 16/500\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.1978 - accuracy: 0.9258\n",
            "Epoch 17/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1927 - accuracy: 0.9271\n",
            "Epoch 18/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1891 - accuracy: 0.9288\n",
            "Epoch 19/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1833 - accuracy: 0.9307\n",
            "Epoch 20/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1792 - accuracy: 0.9329\n",
            "Epoch 21/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1718 - accuracy: 0.9360\n",
            "Epoch 22/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1712 - accuracy: 0.9354\n",
            "Epoch 23/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1662 - accuracy: 0.9378\n",
            "Epoch 24/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1625 - accuracy: 0.9389\n",
            "Epoch 25/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1597 - accuracy: 0.9405\n",
            "Epoch 26/500\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1541 - accuracy: 0.9427\n",
            "Epoch 27/500\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1515 - accuracy: 0.9431\n",
            "Epoch 28/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1496 - accuracy: 0.9444\n",
            "Epoch 29/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1458 - accuracy: 0.9452\n",
            "Epoch 30/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1428 - accuracy: 0.9470\n",
            "Epoch 31/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1369 - accuracy: 0.9489\n",
            "Epoch 32/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1371 - accuracy: 0.9491\n",
            "Epoch 33/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1327 - accuracy: 0.9493\n",
            "Epoch 34/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1311 - accuracy: 0.9509\n",
            "Epoch 35/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1275 - accuracy: 0.9520\n",
            "Epoch 36/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1260 - accuracy: 0.9529\n",
            "Epoch 37/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1218 - accuracy: 0.9543\n",
            "Epoch 38/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1207 - accuracy: 0.9551\n",
            "Epoch 39/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1184 - accuracy: 0.9550\n",
            "Epoch 40/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1168 - accuracy: 0.9559\n",
            "Epoch 41/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1154 - accuracy: 0.9563\n",
            "Epoch 42/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1130 - accuracy: 0.9570\n",
            "Epoch 43/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1123 - accuracy: 0.9583\n",
            "Epoch 44/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1075 - accuracy: 0.9604\n",
            "Epoch 45/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1073 - accuracy: 0.9596\n",
            "Epoch 46/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1051 - accuracy: 0.9610\n",
            "Epoch 47/500\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1044 - accuracy: 0.9608\n",
            "Epoch 48/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1022 - accuracy: 0.9612\n",
            "Epoch 49/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0977 - accuracy: 0.9631\n",
            "Epoch 50/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0982 - accuracy: 0.9636\n",
            "Epoch 51/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0970 - accuracy: 0.9639\n",
            "Epoch 52/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0938 - accuracy: 0.9648\n",
            "Epoch 53/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0934 - accuracy: 0.9649\n",
            "Epoch 54/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0921 - accuracy: 0.9661\n",
            "Epoch 55/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0909 - accuracy: 0.9658\n",
            "Epoch 56/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0894 - accuracy: 0.9666\n",
            "Epoch 57/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0898 - accuracy: 0.9671\n",
            "Epoch 58/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0856 - accuracy: 0.9685\n",
            "Epoch 59/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0842 - accuracy: 0.9680\n",
            "Epoch 60/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0856 - accuracy: 0.9680\n",
            "Epoch 61/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0840 - accuracy: 0.9688\n",
            "Epoch 62/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0808 - accuracy: 0.9692\n",
            "Epoch 63/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0805 - accuracy: 0.9704\n",
            "Epoch 64/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0816 - accuracy: 0.9693\n",
            "Epoch 65/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0800 - accuracy: 0.9692\n",
            "Epoch 66/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0757 - accuracy: 0.9723\n",
            "Epoch 67/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0770 - accuracy: 0.9715\n",
            "Epoch 68/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0745 - accuracy: 0.9722\n",
            "Epoch 69/500\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0743 - accuracy: 0.9718\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4f96347820>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate on test\n",
        "model.evaluate(test_images, test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oWfAa5Fczun",
        "outputId": "e6d979d9-ecfe-4448-aa55-ab6fd59fd33e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.6255 - accuracy: 0.8872\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6255062222480774, 0.8871999979019165]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf = tf.squeeze(model.predict(test_images))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oA3hOXxg-Cg",
        "outputId": "e6e4f9df-72d8-447d-f6ca-7abaadcddf98"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(clf[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahQHEHIIjJcz",
        "outputId": "ebe51099-11dc-4007-bdab-6c4db11a2e13"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([9.6435003e-24 4.9729081e-28 3.1732106e-21 2.2414528e-24 7.9926545e-35 7.0507158e-14 4.3130339e-33 4.4754986e-10 2.7430818e-24 9.9999994e-01], shape=(10,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_labels[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgL7FqALjOzQ",
        "outputId": "06a60d42-a1eb-437a-b35c-b312a4681dc2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8hmy_GZtjpvS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}